!is.na(pmale),
!is.na(p65above)
# !is.na(pwhite),
# !is.na(ppoverty),
# !is.na(phealweight),
# !is.na(pquits),
# !is.na(no_hospitals),
# !is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
# !is.na(ppoverty),
# !is.na(phealweight),
# !is.na(pquits),
# !is.na(no_hospitals),
# !is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
# !is.na(phealweight),
# !is.na(pquits),
# !is.na(no_hospitals),
# !is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
# !is.na(pquits),
# !is.na(no_hospitals),
# !is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
# !is.na(no_hospitals),
# !is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
!is.na(no_hospitals),
# !is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
#!is.na(no_hospitals),
!is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
#!is.na(no_hospitals),
#!is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
#!is.na(no_hospitals),
!is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
!is.na(no_hospitals),
#!is.na(pnative)
) %>%
nrow()
data %>%
filter(
!is.na(pCOPD),
!is.na(psmoke),
!is.na(pmale),
!is.na(p65above),
!is.na(pwhite),
!is.na(ppoverty),
!is.na(phealweight),
!is.na(pquits),
#!is.na(no_hospitals),
#!is.na(pnative)
) %>%
nrow()
View(data)
data %>%
filter(
# !is.na(pCOPD),
# !is.na(psmoke),
# !is.na(pmale),
# !is.na(p65above),
# !is.na(pwhite),
# !is.na(ppoverty),
# !is.na(phealweight),
# !is.na(pquits),
is.na(no_hospitals),
is.na(pnative)
) %>%
nrow()
data %>%
filter(
# !is.na(pCOPD),
# !is.na(psmoke),
# !is.na(pmale),
# !is.na(p65above),
# !is.na(pwhite),
# !is.na(ppoverty),
# !is.na(phealweight),
# !is.na(pquits),
is.na(no_hospitals),
#is.na(pnative)
) %>%
View()
data %>%
filter(
# !is.na(pCOPD),
# !is.na(psmoke),
# !is.na(pmale),
# !is.na(p65above),
# !is.na(pwhite),
# !is.na(ppoverty),
# !is.na(phealweight),
# !is.na(pquits),
#is.na(no_hospitals),
is.na(pnative)
) %>%
View()
library(tidyverse)
library(igraph)
facebook <- read_csv("facebook_combined.csv", col_names = FALSE,
cols(X1 = col_character(),
X2 = col_character()))
graph <- graph_from_edgelist(as.matrix(facebook))
graph.density(graph)
centr_degree(graph)$centralization
centr_clo(graph, mode = "all")$centralization
centr_betw(graph)$centralization
centr_eigen(graph)$centralization
which(centr_degree(graph)$res == max(centr_degree(graph)$res))
which(centr_degree(graph)$res == min(centr_degree(graph)$res))
which(centr_clo(graph, mode = "all")$res == max(centr_clo(graph, mode = "all")$res))
which(centr_clo(graph, mode = "all")$res == min(centr_clo(graph, mode = "all")$res))
which(centr_betw(graph)$res == max(centr_betw(graph)$res))
which(centr_betw(graph)$res == min(centr_betw(graph)$res))
which(centr_eigen(graph)$vector == max(centr_eigen(graph)$vector))
which(centr_eigen(graph)$vector - min(centr_eigen(graph)$vector) < 1e-20)
github <- read_csv("git_edges.csv", col_names = FALSE,
cols(X1 = col_character(),
X2 = col_character()))
graph2 <- graph_from_edgelist(as.matrix(github))
graph.density(graph2)
centr_degree(graph2)$centralization
#centr_clo(graph2, mode = "all")$centralization
components(graph2)$no
centr_betw(graph2)$centralization
centr_eigen(graph2)$centralization
centr_degree(graph2, normalized = TRUE)$centralization
centr_degree(graph2, normalized = FALSE)$centralization
centr_degree(graph2, normalized = TRUE)$centralization
graph2
components(graph2)
github <- read_csv("git_edges.csv", col_names = TRUE,
cols(id_1 = col_character(),
id_2 = col_character()))
graph2 <- graph_from_edgelist(as.matrix(github))
graph.density(graph2)
centr_degree(graph2, normalized = TRUE)$centralization
centr_clo(graph2, mode = "all")$centralization
centr_betw(graph2)$centralization
centr_eigen(graph2)$centralization
library(igraph)
library(tidyverse)
facebook <- read_csv("facebook_combined.csv", col_names = FALSE,
cols(X1 = col_character(),
X2 = col_character()))
graph <- graph_from_edgelist(as.matrix(facebook))
sub <- induced_subgraph(graph, 1:100) %>% as.undirected() # find all connections of the initial 100 nodes
# Girvan-Newman partitioning ----
gn <- edge.betweenness.community(sub)
gn
plot(gn, sub)
gn_memb = data.frame(gn$membership)
summary(gn_memb)
# random walk partitioning ----
walk <- walktrap.community(sub)
walk
plot(walk, sub)
walk_memb <- data.frame(walk$membership)
summary(walk_memb)
name <-data.frame(name = V(sub)$name)
comp <- cbind(dd, gn_memb, walk_memb)
name <-data.frame(name = V(sub)$name)
comp <- cbind(name, gn_memb, walk_memb)
comp
nrow(facebook)
graph.density(sub)
sub
graph.density(sub)
V(sub)$color = walk$membership
plot(sub, vertex.size = degree(sub))
V(sub)$color = walk$membership
plot(sub)
V(sub)$color = gn$membership
plot(gn, sub)
V(sub)$color = walk$membership
plot(sub, walk)
V(sub)$color = gn$membership
plot(gn, sub)
V(sub)$color = walk$membership
plot(walk, sub)
library(igraph)
library(tidyverse)
facebook <- read_csv("facebook_combined.csv", col_names = FALSE,
cols(X1 = col_character(),
X2 = col_character()))
graph <- graph_from_edgelist(as.matrix(facebook))
sub <- induced_subgraph(graph, 1:200) %>% as.undirected() # find all connections of the initial 100 nodes
graph.density(sub)
# Girvan-Newman partitioning ----
gn <- edge.betweenness.community(sub)
gn
V(sub)$color = gn$membership
plot(gn, sub)
gn_memb = data.frame(gn$membership)
summary(gn_memb)
# random walk partitioning ----
walk <- walktrap.community(sub)
walk
V(sub)$color = walk$membership
plot(walk, sub)
walk_memb <- data.frame(walk$membership)
summary(walk_memb)
name <-data.frame(name = V(sub)$name)
comp <- cbind(name, gn_memb, walk_memb)
comp
sub
library(igraph)
library(tidyverse)
facebook <- read_csv("facebook_combined.csv", col_names = FALSE,
cols(X1 = col_character(),
X2 = col_character()))
graph <- graph_from_edgelist(as.matrix(facebook))
sub <- induced_subgraph(graph, 1:300) %>% as.undirected() # find all connections of the initial 100 nodes
graph.density(sub)
# Girvan-Newman partitioning ----
gn <- edge.betweenness.community(sub)
gn
V(sub)$color = gn$membership
plot(gn, sub)
gn_memb = data.frame(gn$membership)
summary(gn_memb)
# random walk partitioning ----
walk <- walktrap.community(sub)
walk
V(sub)$color = walk$membership
plot(walk, sub)
walk_memb <- data.frame(walk$membership)
summary(walk_memb)
name <-data.frame(name = V(sub)$name)
comp <- cbind(name, gn_memb, walk_memb)
comp
sub
graph
library(tidyverse)
dt <- read_csv("complaints_cleaned.csv")
head(dt)
colnames(dt)
library(tidyverse)
dt <- read_csv("complaints_cleaned.csv")
colnames(dt)
library(tidyverse)
library(tm)
library(quanteda)
library(qdap)
library(tidytext)
library(SnowballC)
library(wordcloud)
library(plotrix)
# create term-document matrix ----
tdm <- tidy(TermDocumentMatrix(as.VCorpus(dt)))
head(dt$Issue)
# create term-document matrix ----
tdm <- tidy(TermDocumentMatrix(as.VCorpus(dt$Issue)))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(quanteda)
library(qdap)
library(tidytext)
library(SnowballC)
library(wordcloud)
library(plotrix)
ks <- read_csv("kickstarter_projects_2020_02.csv")
# subset data
most_successful <- ks %>%
arrange(desc(backers_count)) %>%
head(1000) %>%
mutate(
doc_id = paste("success", row_number(), sep = ""),
text = blurb) %>%
select(doc_id, text)
least_successful <- ks %>%
arrange(backers_count) %>%
head(1000) %>%
mutate(
doc_id = paste("failure", row_number(), sep = ""),
text = blurb) %>%
select(doc_id, text)
# create corpus
success_corpus <- VCorpus(DataframeSource(most_successful))
failure_corpus <- VCorpus(DataframeSource(least_successful))
# clean corpus
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(replace_symbol))
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(removeNumPunct))
corpus <- tm_map(corpus, stripWhitespace)
return(corpus)
}
clean_success <- clean_corpus(success_corpus)
clean_failure <- clean_corpus(failure_corpus)
# stem corpus
stemmed_success <- tm_map(clean_success, stemDocument)
stemmed_failure <- tm_map(clean_failure, stemDocument)
# stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
compl_success <- lapply(clean_success, content_transformer(stemCompletion2), dictionary = clean_success)
compl_failure <- lapply(clean_failure, content_transformer(stemCompletion2), dictionary = clean_success)
compl_success
# load libraries ----
library(tidyverse)
library(tm)
library(quanteda)
library(qdap)
library(tidytext)
library(SnowballC)
library(wordcloud)
library(plotrix)
# read data ----
data <- read_csv("complaints_cleaned.csv")
colnames(data)
text <- data %>%
mutate(
doc_id = as.character(row_number()),
text = Issue
)
head(text)
text <- data %>%
mutate(
doc_id = as.character(row_number()),
text = Issue
) %>%
select(
doc_id, text
)
text
issue <- data %>%
mutate(
doc_id = as.character(row_number()),
text = Issue
) %>%
select(
doc_id, text
)
issue_corpus <- VCorpus(DataframeSource(issue))
issue_corpus
issue_stem <- tm_map(issue_corpus, stemDocument)
issue_stem
issue_stem <- tm_map(issue_corpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
issue_compl <- lapply(issue_stem, content_transformer(stemCompletion2), dictionary = issue_corpus)
# create term-document matrix ----
issue_td <- tidy(TermDocumentMatrix(as.VCorpus(issue_compl)))
issue_tf_idf <- issue_td %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# word cloud
set.seed(7)
wordcloud(issue_tf_idf$term, issue_tf_idf$tf, max.words = 50, colors = blues9 ,scale=c(2.5,0.1))
issue_tf_idf$term
issue_td
issue_td %>% arrange(desc(count))
issue_compl
as.VCorpus(issue_compl)
TermDocumentMatrix(as.VCorpus(issue_compl))
# load libraries ----
library(tidyverse)
library(tm)
library(quanteda)
library(qdap)
library(tidytext)
library(SnowballC)
library(wordcloud)
library(plotrix)
# read data ----
data <- read_csv("complaints_cleaned.csv")
colnames(data)
# create corpus ----
issue <- data %>%
mutate(
doc_id = as.character(row_number()),
text = Issue
) %>%
select(
doc_id, text
)
issue_corpus <- VCorpus(DataframeSource(issue))
# clean corpus ----
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(replace_symbol))
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(removeNumPunct))
corpus <- tm_map(corpus, stripWhitespace)
return(corpus)
}
issue_clean <- clean_corpus(issue_corpus)
# stem corpus and stem completion ----
issue_stem <- tm_map(issue_corpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
issue_compl <- lapply(issue_stem, content_transformer(stemCompletion2), dictionary = issue_corpus)
